version: "3.9"

# AI4All â€“ Full Stack
# Start: docker-compose up -d
# Web UI:  http://localhost:3000
# API:     http://localhost:8000/docs

services:

  ollama:
    image: ollama/ollama:latest
    container_name: ai4all-ollama
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Remove the 'deploy' block above if you have no GPU (CPU-only mode)
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:11434/api/tags || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10

  api:
    build:
      context: ./api
    container_name: ai4all-api
    environment:
      - AI4ALL_OLLAMA_URL=http://ollama:11434
      - AI4ALL_NODE_API_URL=http://node:7070
    ports:
      - "8000:8000"
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  webui:
    build:
      context: ./webui
      dockerfile: Dockerfile.prod
    container_name: ai4all-webui
    ports:
      - "3000:80"
    depends_on:
      api:
        condition: service_healthy

  # Pull a default model after Ollama starts
  model-init:
    image: ollama/ollama:latest
    container_name: ai4all-model-init
    entrypoint: >
      sh -c "
        echo 'Waiting for Ollama...' &&
        until curl -sf http://ollama:11434/api/tags; do sleep 2; done &&
        echo 'Pulling llama3...' &&
        ollama pull llama3 &&
        echo 'Pulling phi3 (lightweight)...' &&
        ollama pull phi3 &&
        echo 'Models ready!'
      "
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy
    restart: "no"

volumes:
  ollama-data:
