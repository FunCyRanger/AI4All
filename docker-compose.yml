version: "3.9"

# AI4All â€“ Base Stack (CPU mode)
# Use an override file for GPU acceleration:
#   NVIDIA:  docker compose -f docker-compose.yml -f docker-compose.nvidia.yml up -d
#   AMD:     docker compose -f docker-compose.yml -f docker-compose.amd.yml up -d
#   Auto:    bash setup.sh   (detects your GPU automatically)

services:

  ollama:
    image: ollama/ollama:latest
    container_name: ai4all-ollama
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_KEEP_ALIVE=30m
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
    healthcheck:
      test: ["CMD-SHELL", "ollama list > /dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 20s
    restart: unless-stopped

  node:
    build:
      context: .
      dockerfile: Dockerfile.node
    container_name: ai4all-node
    ports:
      - "7070:7070"
    volumes:
      - ai4all-data:/root/.ai4all
    environment:
      - AI4ALL_LOG_LEVEL=info
    restart: unless-stopped

  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: ai4all-api
    environment:
      - AI4ALL_OLLAMA_URL=http://ollama:11434
      - AI4ALL_NODE_API_URL=http://node:7070
    ports:
      - "8080:8000"
    depends_on:
      ollama:
        condition: service_healthy
      node:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  webui:
    build:
      context: ./webui
      dockerfile: Dockerfile.prod
    container_name: ai4all-webui
    ports:
      - "3000:80"
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped

  model-init:
    image: ollama/ollama:latest
    container_name: ai4all-model-init
    entrypoint: >
      sh -c "
        echo 'Waiting for Ollama...' &&
        until ollama list > /dev/null 2>&1; do sleep 3; done &&
        ollama pull llama3 &&
        ollama pull phi3 &&
        echo 'Models ready!'
      "
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy
    restart: "no"

volumes:
  ollama-data:
  ai4all-data:
