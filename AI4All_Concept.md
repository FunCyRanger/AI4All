# AI4All â€“ Decentralized AI Network for Everyone

> **Vision:** A community-operated AI infrastructure where every user gains access to powerful AI by sharing their computing resources â€“ no central providers, no dependencies, no gatekeepers.

---

## 1. Core Principles

| Principle | Description |
|---|---|
| **Decentralization** | No central server. Every node is equal. |
| **Fairness** | Token system ensures: those who contribute, benefit. |
| **Privacy** | Requests are split, encrypted, and never fully exposed to a single node. |
| **Common Good** | Open source, non-profit, community governance. |
| **Legal Compliance** | GDPR-compliant, no personal data stored on third-party devices. |

---

## 2. System Architecture (Overview)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AI4All Network                          â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Node A  â”‚â—„â”€â”€â–ºâ”‚  Node B  â”‚â—„â”€â”€â–ºâ”‚  Node C  â”‚  ...        â”‚
â”‚  â”‚(Raspberryâ”‚    â”‚ (Windows â”‚    â”‚  (Linux  â”‚              â”‚
â”‚  â”‚   Pi)    â”‚    â”‚  PC)     â”‚    â”‚  Server) â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚               â”‚               â”‚                     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                    P2P Mesh                                  â”‚
â”‚              (libp2p / WebRTC)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                            â”‚
   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
   â”‚   Web UI   â”‚              â”‚    API     â”‚
   â”‚(OpenWebUI- â”‚              â”‚  (REST /   â”‚
   â”‚  based)    â”‚              â”‚ GraphQL)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Components in Detail

### 3.1 AI4All Node (Client)

The client is the heart of the system. It runs on the user's device and fulfills multiple roles simultaneously.

**Platform Support:**
- Windows (Electron app or native binary)
- Linux (AppImage / Snap / deb / rpm)
- macOS (Universal Binary for Intel + Apple Silicon)
- Android (background service, restricted mode)
- Raspberry Pi / ARM devices (lightweight mode)

**Node Technology Stack:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AI4All Node             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Inference Engine               â”‚
â”‚  â†’ llama.cpp (CPU/GPU/Metal)    â”‚
â”‚  â†’ ONNX Runtime                 â”‚
â”‚  â†’ MLC-LLM (mobile)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model Sharding Layer           â”‚
â”‚  â†’ Tensor Slicing per Layer     â”‚
â”‚  â†’ Pipeline Parallelism         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  P2P Network Layer              â”‚
â”‚  â†’ libp2p (Go/Rust)             â”‚
â”‚  â†’ Kademlia DHT                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Token Accounting               â”‚
â”‚  â†’ local wallet                 â”‚
â”‚  â†’ signed transactions          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Security Sandbox               â”‚
â”‚  â†’ WASM Isolation               â”‚
â”‚  â†’ Resource Limits              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Model Sharding (Distributed Memory)

The core technical challenge: LLMs are too large for individual devices.

**Solution â€“ Pipeline Parallelism:**
```
Model (e.g. LLaMA 70B, ~40GB)
â”‚
â”œâ”€â”€ Layers  0â€“20  â”€â”€â–º  Node A (16 GB RAM)
â”œâ”€â”€ Layers 21â€“40  â”€â”€â–º  Node B  (8 GB RAM)
â””â”€â”€ Layers 41â€“80  â”€â”€â–º  Node C (16 GB RAM)

Activations are encrypted and passed between nodes.
```

**Technical Approach:**
- Inspired by **Petals** (BitTorrent for LLMs) â€“ already proven in practice
- Nodes only store parts of a model (layer chunks)
- Activations (not the prompt itself) travel through the layer chain
- The prompt is encrypted and only decrypted at the first layer

**Alternative Approach â€“ Tensor Parallelism:**
- Width-based splitting (weight matrices are partitioned)
- Higher communication overhead, but better suited for homogeneous networks

### 3.3 Token System (Fairness Mechanism)

The token system is not blockchain-based (too energy-intensive), but relies on a **signed reputation ledger**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           AI4All Token System              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                            â”‚
â”‚  Providing compute resources               â”‚
â”‚  â†’ +tokens per processed inference unit   â”‚
â”‚  â†’ bonus for high availability (uptime)    â”‚
â”‚  â†’ bonus for hosting rare model layers     â”‚
â”‚                                            â”‚
â”‚  Consuming compute resources               â”‚
â”‚  â†’ -tokens per request (by complexity)     â”‚
â”‚  â†’ lower cost for higher contributors      â”‚
â”‚                                            â”‚
â”‚  Fairness Rules                            â”‚
â”‚  â†’ maximum token accumulation is capped   â”‚
â”‚  â†’ new users receive starter tokens        â”‚
â”‚  â†’ inactive nodes do not lose tokens       â”‚
â”‚    (they just stop earning new ones)       â”‚
â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ledger Technology:** No full blockchain overhead required. Instead:
- **Signed receipts** between nodes (similar to IOTA / Directed Acyclic Graphs)
- Periodic consensus rounds via a gossip protocol
- Decentralized validator nodes (elected from long-standing community members)

### 3.4 Privacy & Security

**Core challenge:** How do we prevent a node from reading another user's prompt?

**Solutions:**

1. **Prompt Fragmentation:** The prompt is split into fragments processed by different nodes â€“ no single node ever knows the full context.

2. **Homomorphic Encryption (long-term):** Computation on encrypted data â€“ still too slow for production use, but an active research area.

3. **Trusted Execution Environments (TEE):** AMD SEV or Intel TDX â€“ computations happen inside an isolated enclave that cannot be read from outside.

4. **Differential Privacy:** Activations passed between nodes are slightly noised in a way that does not distort the result but prevents reverse-engineering.

5. **WASM Sandbox for Node Code:** Every computation task runs inside an isolated WebAssembly sandbox â€“ no access to the host system.

**Additional Safeguards:**
- Nodes only see their own layer activations, never the plaintext prompt
- No storage of requests on third-party devices
- Resource sharing is always opt-in â€“ users retain full control
- Rate limiting to prevent abuse
- Reputation system: misbehaving nodes are excluded from the network

### 3.5 P2P Network

**Technology:** libp2p (the same foundation as IPFS and Ethereum)

```
Discovery:
  â†’ Kademlia DHT for node discovery
  â†’ mDNS for local networks (LAN boost)

Transport:
  â†’ QUIC (primary, NAT-traversal capable)
  â†’ WebRTC (browser compatibility)
  â†’ TCP (fallback)

Routing:
  â†’ Requests are routed to nodes
    that hold the required model layers
  â†’ Latency-optimized routing
    (geographic proximity preferred)
```

---

## 4. Model Categories & Specializations

```
AI4All Model Registry (decentralized, via IPFS / Arweave)

â”œâ”€â”€ ğŸ–¼ï¸  Vision & Media
â”‚   â”œâ”€â”€ LLaVA / Moondream (image analysis)
â”‚   â”œâ”€â”€ VideoLLaMA (video analysis)
â”‚   â””â”€â”€ Whisper (audio transcription)
â”‚
â”œâ”€â”€ ğŸ’»  Code & Development
â”‚   â”œâ”€â”€ CodeLlama 34B
â”‚   â”œâ”€â”€ DeepSeek Coder
â”‚   â””â”€â”€ Qwen2.5-Coder
â”‚
â”œâ”€â”€ ğŸ”¬  Science & Knowledge
â”‚   â”œâ”€â”€ LLaMA 3.1 70B (general purpose)
â”‚   â”œâ”€â”€ Meditron (medicine / health)
â”‚   â””â”€â”€ LegalBERT derivatives (law)
â”‚
â”œâ”€â”€ ğŸŒ  Research & Web
â”‚   â”œâ”€â”€ Model + integrated web search agent
â”‚   â””â”€â”€ RAG pipeline over SearXNG (self-hosted)
â”‚
â””â”€â”€ ğŸ—£ï¸  Multilingual
    â”œâ”€â”€ BLOOM derivatives
    â””â”€â”€ Aya (Cohere, 100+ languages)
```

---

## 5. Web UI & API

### Web UI (Based on Open WebUI)

Open WebUI is the ideal foundation because:
- Already fully compatible with Ollama
- Supports multi-model selection out of the box
- Active community, MIT license
- Extensible via plugins and custom functions

**AI4All-specific extensions:**
```
Open WebUI
â”œâ”€â”€ + Token balance display (header)
â”œâ”€â”€ + Node status dashboard
â”œâ”€â”€ + Model category selection (icons)
â”œâ”€â”€ + Privacy indicator per request
â”œâ”€â”€ + Network contribution settings
â””â”€â”€ + Community models (from decentralized registry)
```

### REST API

```yaml
# Compatible with OpenAI API schema
POST /v1/chat/completions
  â†’ model: "ai4all/codellama-34b"
  â†’ messages: [...]
  â†’ response: streaming SSE

GET /v1/models
  â†’ list of all available community models

GET /v1/tokens/balance
  â†’ current token balance

GET /v1/node/status
  â†’ own contribution, uptime, active layers
```

---

## 6. Technology Stack Summary

| Layer | Technology | Reason |
|---|---|---|
| Inference | llama.cpp, MLC-LLM | CPU/GPU/Metal, all platforms |
| Model Sharding | Petals concept (custom impl.) | Proven, open source |
| P2P Network | libp2p (Go or Rust) | Industry standard, IPFS-proven |
| Node Client | Rust (core) + Electron (GUI) | Performance + cross-platform |
| Web UI | Open WebUI (fork) | Feature-rich, active community |
| API | FastAPI (Python) or Axum (Rust) | Simple, performant |
| Token Ledger | DAG + Gossip (custom) | No blockchain overhead |
| Model Registry | IPFS + Arweave | Decentralized, immutable |
| Security | WASM Sandbox, TEE (optional) | Battle-tested, extensible |
| Authentication | DID (Decentralized Identity) | No central login required |

---

## 7. Development Roadmap

### Phase 1 â€“ MVP (Months 0â€“6)
- [ ] Node client for Linux & Windows
- [ ] Basic P2P network (libp2p)
- [ ] Single model (LLaMA 3.1 8B) distributed across 2â€“4 nodes
- [ ] Simple token system (without consensus)
- [ ] Open WebUI integration
- [ ] GitHub repository + documentation

### Phase 2 â€“ Community Beta (Months 6â€“12)
- [ ] Model sharding for models up to 70B
- [ ] macOS & Android client
- [ ] Token consensus mechanism
- [ ] Model registry (IPFS)
- [ ] Specialized model categories
- [ ] Privacy layer (prompt fragmentation)

### Phase 3 â€“ Stabilization (Months 12â€“24)
- [ ] TEE integration (AMD SEV)
- [ ] Web search agent (SearXNG)
- [ ] Community governance (model voting)
- [ ] Mobile-optimized light node
- [ ] Security audit

---

## 8. Comparison with Existing Projects

| Project | Approach | AI4All Difference |
|---|---|---|
| **Petals** | Layer sharing for LLMs | Our main inspiration â€“ extended with token system, UI, broader platform support |
| **Bittensor** | Blockchain-based | Too complex, high energy overhead from consensus |
| **Ollama** | Local, no P2P | No community aspect |
| **Golem** | General computing | Not AI-specific, complex setup |
| **IPFS** | File sharing | Storage only, no inference |

**AI4All is closest to Petals** â€“ but focused on:
- User-friendliness (GUI, WebUI)
- Fairness mechanism (tokens)
- Privacy by design
- Broad platform support

---

## 9. GitHub Repository Structure

```
AI4All/
â”œâ”€â”€ core/                  # Rust: P2P, inference, token logic
â”‚   â”œâ”€â”€ node/              # Node daemon
â”‚   â”œâ”€â”€ inference/         # llama.cpp bindings
â”‚   â”œâ”€â”€ network/           # libp2p integration
â”‚   â””â”€â”€ tokens/            # Token accounting
â”œâ”€â”€ clients/
â”‚   â”œâ”€â”€ desktop/           # Electron app
â”‚   â”œâ”€â”€ android/           # Android service
â”‚   â””â”€â”€ cli/               # Command line interface
â”œâ”€â”€ api/                   # FastAPI gateway
â”œâ”€â”€ webui/                 # Open WebUI fork
â”œâ”€â”€ registry/              # Decentralized model registry
â”œâ”€â”€ docs/                  # Documentation
â””â”€â”€ scripts/               # Setup & deployment
```

---

## 10. Open Challenges

**Latency:** Pipeline parallelism introduces network overhead. Realistic response times for 70B models are 5â€“30 seconds. Acceptable for many use cases, but worth communicating clearly to users.

**Node availability:** The network needs a critical mass of roughly 100+ active nodes to be reliable. Community building is crucial â€“ a small number of dedicated seed servers at launch would help bootstrap this.

**Model updates:** When a model is updated, all nodes storing its layers need to synchronize. A coordinated rolling update mechanism is required.

**Licensing:** Model licenses (e.g. LLaMA Community License) restrict commercial use. AI4All must be explicitly positioned as non-commercial to remain compliant.

**Abuse prevention:** The token system must resist Sybil attacks (many fake nodes). A lightweight proof-of-contribution or human verification step will be necessary.

---

## 11. Contributing

AI4All is built by and for the community. All contributions are welcome:

- **Developers:** Pick up issues on GitHub, improve the core node, or extend the Web UI
- **Node operators:** Run a node and help grow the network
- **Researchers:** Improve privacy mechanisms, sharding efficiency, or consensus design
- **Translators:** Help make the documentation accessible in more languages
- **Advocates:** Spread the word and help build the community

Please read `CONTRIBUTING.md` before submitting your first pull request.

---

*AI4All â€“ AI belongs to everyone.*
*License: Apache 2.0 | Governance: Community DAO*
