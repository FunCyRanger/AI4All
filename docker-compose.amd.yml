# AI4All – AMD GPU Override (ROCm)
# Usage: docker compose -f docker-compose.yml -f docker-compose.amd.yml up -d
#
# Requirements:
#   - AMD GPU with ROCm support (RX 5000 series and newer recommended)
#   - ROCm driver: https://rocm.docs.amd.com/en/latest/deploy/linux/index.html
#     Quick install (Ubuntu 22.04/24.04):
#       wget https://repo.radeon.com/amdgpu-install/6.1.3/ubuntu/jammy/amdgpu-install_6.1.60103-1_all.deb
#       sudo dpkg -i amdgpu-install_6.1.60103-1_all.deb
#       sudo amdgpu-install --usecase=rocm
#       sudo usermod -aG render,video $USER && newgrp render
#
# Supported GFX architectures:
#   gfx906  – Radeon VII / MI50 / MI60
#   gfx908  – MI100
#   gfx90a  – MI200 series
#   gfx942  – MI300 series
#   gfx1030 – RX 6800 / 6900 (RDNA 2)
#   gfx1100 – RX 7900 series (RDNA 3)  ← most common consumer card
#   gfx1101 – RX 7700 / 7800 XT
#   gfx1102 – RX 7600

services:

  ollama:
    image: ollama/ollama:rocm     # ROCm-enabled image from Docker Hub
    devices:
      # KFD: Kernel Fusion Driver – required for ROCm compute
      - /dev/kfd:/dev/kfd
      # DRI: Direct Rendering Infrastructure – GPU device files
      - /dev/dri:/dev/dri
    group_add:
      - video   # required to access DRI devices
      - render  # required for ROCm on newer kernels
    environment:
      # ROCm GPU visibility
      - HIP_VISIBLE_DEVICES=0         # change to "0,1" for multiple GPUs
      - ROCR_VISIBLE_DEVICES=0
      # GFX version override – IMPORTANT for consumer cards not in ROCm's official list
      # Common values:
      #   RDNA 3 (RX 7xxx):  HSA_OVERRIDE_GFX_VERSION=11.0.0
      #   RDNA 2 (RX 6xxx):  HSA_OVERRIDE_GFX_VERSION=10.3.0
      #   RDNA 1 (RX 5xxx):  HSA_OVERRIDE_GFX_VERSION=10.1.0
      - HSA_OVERRIDE_GFX_VERSION=11.0.0
      # Disable LLVM DMA for stability on some cards
      - HSA_ENABLE_SDMA=0
      # Ollama settings
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=30m
      - OLLAMA_GPU_MEMORY_FRACTION=0.85
    volumes:
      - ollama-data:/root/.ollama

  model-init:
    entrypoint: >
      sh -c "
        echo 'Waiting for Ollama (AMD ROCm mode)...' &&
        until curl -sf http://ollama:11434/api/tags; do sleep 3; done &&
        echo 'Pulling llama3 (Q4_K_M)...' &&
        ollama pull llama3 &&
        echo 'Pulling phi3 (lightweight – fits in 8GB VRAM)...' &&
        ollama pull phi3 &&
        echo 'Pulling codellama...' &&
        ollama pull codellama &&
        echo 'All AMD models ready!'
      "
    environment:
      - OLLAMA_HOST=http://ollama:11434
